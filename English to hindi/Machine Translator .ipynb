{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jump.\tउछलो.\n",
      "['Help!', 'बचाओ!']\n"
     ]
    }
   ],
   "source": [
    "# Assign the data path.\n",
    "data_path = \"hin.txt\"\n",
    "\n",
    "# Read in the data.\n",
    "lines = io.open(data_path, encoding = \"utf-8\").read().split(\"\\n\")\n",
    "lines  = lines[:-1]\n",
    "print(lines[1])\n",
    "# Split the data into input and target sequences.\n",
    "lines = [line.split(\"\\t\") for line in lines]\n",
    "print(lines[0])\n",
    "# We define the starting signal to be \"\\t\" and the\n",
    "# ending signal to be \"\\n\". These signals tell the\n",
    "# model that when it sees \"\\t\" it should start\n",
    "# producing its translation and produce \"\\n\" when\n",
    "# it wants to end its translation. Let us add\n",
    "# \"\\t\" to the start and \"\\n\" to the end \n",
    "# of all input and output sentences.\n",
    "lines = [(\"\\t\" + line[0] + \"\\n\", \"\\t\" + line[1] + \"\\n\") for\n",
    "            line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tबचाओ!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (lines[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure out the Best Lengths of Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sentence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the input and output lengths.\n",
    "input_lengths = np.array([len(line[0]) for line in lines])\n",
    "output_lengths = np.array([len(line[1]) for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2869\n"
     ]
    }
   ],
   "source": [
    "print(len(input_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75, 80, 0, 120]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(input_lengths)\n",
    "plt.axis([75,80, 0 , 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85, 89, 0, 20]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(output_lengths)\n",
    "plt.axis([85,89,0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = 78\n",
    "hindi = 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = []\n",
    "for i in range(len(input_lengths)):\n",
    "    if(input_lengths[i]<75 and output_lengths[i]<85):\n",
    "        line1 = line1 + [lines[i]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2856\n"
     ]
    }
   ],
   "source": [
    "print(len(line1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted the histogram of the length of the input sentences and choose the length that makes the most sense. \n",
    "\n",
    "The reason we don't want sentences that are too long is because the computation becomes trickier for longer sentences and the performance also degrades. However we also want as many sentences in our dataset as possible.\n",
    "\n",
    "Thus it is important to choose the right length and discard sentences longer than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same for the lengths of the output sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 2869  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [(line[0]) for line in line1]\n",
    "target_texts = [(line[1]) for line in line1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = set()\n",
    "target_characters = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in input_texts:\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "for target_text in target_texts:\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(input_characters))\n",
    "print(len(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2856\n",
      "Number of unique input tokens: 72\n",
      "Number of unique output tokens: 92\n",
      "Max sequence length for inputs: 74\n",
      "Max sequence length for outputs: 82\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            \n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2284 samples, validate on 572 samples\n",
      "Epoch 1/200\n",
      "2284/2284 [==============================] - 24s 10ms/step - loss: 1.1518 - val_loss: 1.8910\n",
      "Epoch 2/200\n",
      "2284/2284 [==============================] - 23s 10ms/step - loss: 1.1064 - val_loss: 1.7642\n",
      "Epoch 3/200\n",
      "2284/2284 [==============================] - 20s 9ms/step - loss: 1.0220 - val_loss: 1.6587\n",
      "Epoch 4/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.9376 - val_loss: 1.5321\n",
      "Epoch 5/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.8756 - val_loss: 1.4986\n",
      "Epoch 6/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.8321 - val_loss: 1.3841\n",
      "Epoch 7/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.7887 - val_loss: 1.3275\n",
      "Epoch 8/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.7597 - val_loss: 1.2994\n",
      "Epoch 9/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.7365 - val_loss: 1.2744\n",
      "Epoch 10/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.7186 - val_loss: 1.2503\n",
      "Epoch 11/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.7018 - val_loss: 1.2325\n",
      "Epoch 12/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.6888 - val_loss: 1.2061\n",
      "Epoch 13/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.6813 - val_loss: 1.2014\n",
      "Epoch 14/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.6648 - val_loss: 1.1856\n",
      "Epoch 15/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.6536 - val_loss: 1.1745\n",
      "Epoch 16/200\n",
      "2284/2284 [==============================] - 17s 7ms/step - loss: 0.6485 - val_loss: 1.1702\n",
      "Epoch 17/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.6341 - val_loss: 1.1541\n",
      "Epoch 18/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.6355 - val_loss: 1.1634\n",
      "Epoch 19/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.6173 - val_loss: 1.1478\n",
      "Epoch 20/200\n",
      "2284/2284 [==============================] - 17s 7ms/step - loss: 0.6074 - val_loss: 1.1312\n",
      "Epoch 21/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.5988 - val_loss: 1.1298\n",
      "Epoch 22/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.5907 - val_loss: 1.1311\n",
      "Epoch 23/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.5827 - val_loss: 1.1089\n",
      "Epoch 24/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.5735 - val_loss: 1.1019\n",
      "Epoch 25/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.5665 - val_loss: 1.0998\n",
      "Epoch 26/200\n",
      "2284/2284 [==============================] - 17s 7ms/step - loss: 0.5595 - val_loss: 1.1174\n",
      "Epoch 27/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.5513 - val_loss: 1.1034\n",
      "Epoch 28/200\n",
      "2284/2284 [==============================] - 18s 8ms/step - loss: 0.5554 - val_loss: 1.1045\n",
      "Epoch 29/200\n",
      "2284/2284 [==============================] - 17s 8ms/step - loss: 0.5367 - val_loss: 1.0966\n",
      "Epoch 30/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.5329 - val_loss: 1.0910\n",
      "Epoch 31/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.5233 - val_loss: 1.0979\n",
      "Epoch 32/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.5567 - val_loss: 1.0889\n",
      "Epoch 33/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.5233 - val_loss: 1.0744\n",
      "Epoch 34/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.5102 - val_loss: 1.0686\n",
      "Epoch 35/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.4998 - val_loss: 1.1057\n",
      "Epoch 36/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.4920 - val_loss: 1.0901\n",
      "Epoch 37/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.4841 - val_loss: 1.0939\n",
      "Epoch 38/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.4764 - val_loss: 1.1041\n",
      "Epoch 39/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.4698 - val_loss: 1.0959\n",
      "Epoch 40/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.4626 - val_loss: 1.0873\n",
      "Epoch 41/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.4552 - val_loss: 1.1017\n",
      "Epoch 42/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.4482 - val_loss: 1.0999\n",
      "Epoch 43/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.4456 - val_loss: 1.0978\n",
      "Epoch 44/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.4336 - val_loss: 1.1159\n",
      "Epoch 45/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.4275 - val_loss: 1.1370\n",
      "Epoch 46/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.4206 - val_loss: 1.1517\n",
      "Epoch 47/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.4132 - val_loss: 1.1377\n",
      "Epoch 48/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.4060 - val_loss: 1.1456\n",
      "Epoch 49/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3997 - val_loss: 1.1396\n",
      "Epoch 50/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3930 - val_loss: 1.1674\n",
      "Epoch 51/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3873 - val_loss: 1.1663\n",
      "Epoch 52/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.3808 - val_loss: 1.1738\n",
      "Epoch 53/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3742 - val_loss: 1.1876\n",
      "Epoch 54/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.3680 - val_loss: 1.1937\n",
      "Epoch 55/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3611 - val_loss: 1.2197\n",
      "Epoch 56/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.3555 - val_loss: 1.1919\n",
      "Epoch 57/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3481 - val_loss: 1.2022\n",
      "Epoch 58/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.3949 - val_loss: 1.1756\n",
      "Epoch 59/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3565 - val_loss: 1.2286\n",
      "Epoch 60/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3444 - val_loss: 1.2187\n",
      "Epoch 61/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3362 - val_loss: 1.2353\n",
      "Epoch 62/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3279 - val_loss: 1.2610\n",
      "Epoch 63/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3209 - val_loss: 1.2603\n",
      "Epoch 64/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3149 - val_loss: 1.2722\n",
      "Epoch 65/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.3101 - val_loss: 1.2927\n",
      "Epoch 66/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.3032 - val_loss: 1.3223\n",
      "Epoch 67/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.2994 - val_loss: 1.2964\n",
      "Epoch 68/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2939 - val_loss: 1.3140\n",
      "Epoch 69/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2892 - val_loss: 1.3328\n",
      "Epoch 70/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2844 - val_loss: 1.3401\n",
      "Epoch 71/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2788 - val_loss: 1.3609\n",
      "Epoch 72/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2763 - val_loss: 1.3792\n",
      "Epoch 73/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2700 - val_loss: 1.3571\n",
      "Epoch 74/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2661 - val_loss: 1.3744\n",
      "Epoch 75/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2618 - val_loss: 1.4097\n",
      "Epoch 76/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2573 - val_loss: 1.3906\n",
      "Epoch 77/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2540 - val_loss: 1.4253\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.2517 - val_loss: 1.4360\n",
      "Epoch 79/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.2466 - val_loss: 1.4299\n",
      "Epoch 80/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.2414 - val_loss: 1.4665\n",
      "Epoch 81/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2381 - val_loss: 1.4747\n",
      "Epoch 82/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2359 - val_loss: 1.4819\n",
      "Epoch 83/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2316 - val_loss: 1.4939\n",
      "Epoch 84/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2290 - val_loss: 1.5155\n",
      "Epoch 85/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2260 - val_loss: 1.4982\n",
      "Epoch 86/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2216 - val_loss: 1.5012\n",
      "Epoch 87/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2197 - val_loss: 1.5326\n",
      "Epoch 88/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2166 - val_loss: 1.5340\n",
      "Epoch 89/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2133 - val_loss: 1.5549\n",
      "Epoch 90/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2103 - val_loss: 1.5559\n",
      "Epoch 91/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2164 - val_loss: 1.5688\n",
      "Epoch 92/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2053 - val_loss: 1.5713\n",
      "Epoch 93/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2027 - val_loss: 1.5983\n",
      "Epoch 94/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.2041 - val_loss: 1.5987\n",
      "Epoch 95/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1973 - val_loss: 1.6147\n",
      "Epoch 96/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.1947 - val_loss: 1.6257\n",
      "Epoch 97/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1932 - val_loss: 1.6315\n",
      "Epoch 98/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.1914 - val_loss: 1.6488\n",
      "Epoch 99/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1885 - val_loss: 1.6434\n",
      "Epoch 100/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.1873 - val_loss: 1.6564\n",
      "Epoch 101/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.1843 - val_loss: 1.6582\n",
      "Epoch 102/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1829 - val_loss: 1.7053\n",
      "Epoch 103/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1804 - val_loss: 1.6742\n",
      "Epoch 104/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1788 - val_loss: 1.6770\n",
      "Epoch 105/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1772 - val_loss: 1.7120\n",
      "Epoch 106/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1862 - val_loss: 1.6971\n",
      "Epoch 107/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1774 - val_loss: 1.7236\n",
      "Epoch 108/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1728 - val_loss: 1.7527\n",
      "Epoch 109/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1697 - val_loss: 1.7539\n",
      "Epoch 110/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1675 - val_loss: 1.7554\n",
      "Epoch 111/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1663 - val_loss: 1.7444\n",
      "Epoch 112/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1638 - val_loss: 1.7785\n",
      "Epoch 113/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1605 - val_loss: 1.7819\n",
      "Epoch 114/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.2088 - val_loss: 1.7443\n",
      "Epoch 115/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1604 - val_loss: 1.7587\n",
      "Epoch 116/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1583 - val_loss: 1.7821\n",
      "Epoch 117/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1560 - val_loss: 1.7959\n",
      "Epoch 118/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1545 - val_loss: 1.8086\n",
      "Epoch 119/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1532 - val_loss: 1.8250\n",
      "Epoch 120/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1517 - val_loss: 1.8188\n",
      "Epoch 121/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1506 - val_loss: 1.8057\n",
      "Epoch 122/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1484 - val_loss: 1.8219\n",
      "Epoch 123/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1475 - val_loss: 1.8605\n",
      "Epoch 124/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1463 - val_loss: 1.8387\n",
      "Epoch 125/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1454 - val_loss: 1.8379\n",
      "Epoch 126/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.1427 - val_loss: 1.8691\n",
      "Epoch 127/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1427 - val_loss: 1.8653\n",
      "Epoch 128/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1419 - val_loss: 1.8970\n",
      "Epoch 129/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1401 - val_loss: 1.8752\n",
      "Epoch 130/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.1391 - val_loss: 1.9146\n",
      "Epoch 131/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1381 - val_loss: 1.8963\n",
      "Epoch 132/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1368 - val_loss: 1.8990\n",
      "Epoch 133/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1362 - val_loss: 1.8844\n",
      "Epoch 134/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1349 - val_loss: 1.8984\n",
      "Epoch 135/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1336 - val_loss: 1.9225\n",
      "Epoch 136/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1327 - val_loss: 1.9412\n",
      "Epoch 137/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1321 - val_loss: 1.9223\n",
      "Epoch 138/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1303 - val_loss: 1.9382\n",
      "Epoch 139/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1297 - val_loss: 1.9557\n",
      "Epoch 140/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1291 - val_loss: 1.9460\n",
      "Epoch 141/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1287 - val_loss: 1.9545\n",
      "Epoch 142/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1264 - val_loss: 1.9586\n",
      "Epoch 143/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1644 - val_loss: 1.9473\n",
      "Epoch 144/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1704 - val_loss: 1.9296\n",
      "Epoch 145/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.1460 - val_loss: 1.9565\n",
      "Epoch 146/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1386 - val_loss: 1.9580\n",
      "Epoch 147/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1287 - val_loss: 1.9588\n",
      "Epoch 148/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1259 - val_loss: 1.9682\n",
      "Epoch 149/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1242 - val_loss: 1.9694\n",
      "Epoch 150/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1343 - val_loss: 1.9739\n",
      "Epoch 151/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1214 - val_loss: 1.9797\n",
      "Epoch 152/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.1210 - val_loss: 1.9633\n",
      "Epoch 153/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1207 - val_loss: 1.9889\n",
      "Epoch 154/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1201 - val_loss: 1.9886\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1189 - val_loss: 2.0256\n",
      "Epoch 156/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1187 - val_loss: 2.0120\n",
      "Epoch 157/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1373 - val_loss: 2.0203\n",
      "Epoch 158/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1883 - val_loss: 1.9657\n",
      "Epoch 159/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1343 - val_loss: 1.9627\n",
      "Epoch 160/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1288 - val_loss: 2.0115\n",
      "Epoch 161/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1195 - val_loss: 2.0385\n",
      "Epoch 162/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1176 - val_loss: 2.0446\n",
      "Epoch 163/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1167 - val_loss: 2.0435\n",
      "Epoch 164/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1152 - val_loss: 2.0332\n",
      "Epoch 165/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1142 - val_loss: 2.0429\n",
      "Epoch 166/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1140 - val_loss: 2.0375\n",
      "Epoch 167/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1127 - val_loss: 2.0355\n",
      "Epoch 168/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1121 - val_loss: 2.0576\n",
      "Epoch 169/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1115 - val_loss: 2.0708\n",
      "Epoch 170/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1109 - val_loss: 2.0674\n",
      "Epoch 171/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1101 - val_loss: 2.0906\n",
      "Epoch 172/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1097 - val_loss: 2.0499\n",
      "Epoch 173/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1088 - val_loss: 2.0574\n",
      "Epoch 174/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1087 - val_loss: 2.0667\n",
      "Epoch 175/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.1082 - val_loss: 2.0654\n",
      "Epoch 176/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.1073 - val_loss: 2.0827\n",
      "Epoch 177/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1069 - val_loss: 2.0873\n",
      "Epoch 178/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1063 - val_loss: 2.0836\n",
      "Epoch 179/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1057 - val_loss: 2.1056\n",
      "Epoch 180/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1046 - val_loss: 2.0977\n",
      "Epoch 181/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1047 - val_loss: 2.0967\n",
      "Epoch 182/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1039 - val_loss: 2.1151\n",
      "Epoch 183/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1033 - val_loss: 2.1209\n",
      "Epoch 184/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1030 - val_loss: 2.1048\n",
      "Epoch 185/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1025 - val_loss: 2.1183\n",
      "Epoch 186/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1017 - val_loss: 2.1176\n",
      "Epoch 187/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.1012 - val_loss: 2.0995\n",
      "Epoch 188/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1011 - val_loss: 2.1373\n",
      "Epoch 189/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1001 - val_loss: 2.1091\n",
      "Epoch 190/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.1000 - val_loss: 2.1237\n",
      "Epoch 191/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.0992 - val_loss: 2.1643\n",
      "Epoch 192/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.0987 - val_loss: 2.1457\n",
      "Epoch 193/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.0989 - val_loss: 2.1455\n",
      "Epoch 194/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.0981 - val_loss: 2.1538\n",
      "Epoch 195/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.0980 - val_loss: 2.1440\n",
      "Epoch 196/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.0970 - val_loss: 2.1595\n",
      "Epoch 197/200\n",
      "2284/2284 [==============================] - 15s 6ms/step - loss: 0.0972 - val_loss: 2.1500\n",
      "Epoch 198/200\n",
      "2284/2284 [==============================] - 14s 6ms/step - loss: 0.0967 - val_loss: 2.1604\n",
      "Epoch 199/200\n",
      "2284/2284 [==============================] - 15s 7ms/step - loss: 0.0967 - val_loss: 2.1328\n",
      "Epoch 200/200\n",
      "2284/2284 [==============================] - 16s 7ms/step - loss: 0.0957 - val_loss: 2.1578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2386152f6a0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,batch_size=batch_size,epochs=epochs,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aane\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('s2s.h5')\n",
    "model.save_weights('seq2seq.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "               stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "\tI love you.\n",
      "\n",
      "मैं उससे मिलना चाहूँगा।\n",
      "\n",
      "-\n",
      "\tI love you.\n",
      "\n",
      "मैं उससे मिलना चाहूँगा।\n",
      "\n",
      "-\n",
      "\tI will try.\n",
      "\n",
      "मैं अंग्रेज़ी पढ़ सकता हूँ।\n",
      "\n",
      "-\n",
      "\tI'm coming.\n",
      "\n",
      "मैं अंग्रेज़ी पढ़ सकता हूँ।\n",
      "\n",
      "-\n",
      "\tI'm hungry!\n",
      "\n",
      "मैं उससे मिलना चाहूँगा।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(55,60):\n",
    "    \n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print(input_texts[seq_index])\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
