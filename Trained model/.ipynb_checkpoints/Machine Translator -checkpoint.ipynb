{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aane\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jump.\tउछलो.\n",
      "['Help!', 'बचाओ!']\n"
     ]
    }
   ],
   "source": [
    "# Assign the data path.\n",
    "data_path = \"hin.txt\"\n",
    "\n",
    "# Read in the data.\n",
    "lines = io.open(data_path, encoding = \"utf-8\").read().split(\"\\n\")\n",
    "lines  = lines[:-1]\n",
    "print(lines[1])\n",
    "# Split the data into input and target sequences.\n",
    "lines = [line.split(\"\\t\") for line in lines]\n",
    "print(lines[0])\n",
    "# We define the starting signal to be \"\\t\" and the\n",
    "# ending signal to be \"\\n\". These signals tell the\n",
    "# model that when it sees \"\\t\" it should start\n",
    "# producing its translation and produce \"\\n\" when\n",
    "# it wants to end its translation. Let us add\n",
    "# \"\\t\" to the start and \"\\n\" to the end \n",
    "# of all input and output sentences.\n",
    "lines = [(\"\\t\" + line[0] + \"\\n\", \"\\t\" + line[1] + \"\\n\") for\n",
    "            line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tबचाओ!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (lines[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure out the Best Lengths of Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sentence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the input and output lengths.\n",
    "input_lengths = np.array([len(line[0]) for line in lines])\n",
    "output_lengths = np.array([len(line[1]) for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2869\n"
     ]
    }
   ],
   "source": [
    "print(len(input_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75, 80, 0, 120]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(input_lengths)\n",
    "plt.axis([75,80, 0 , 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85, 89, 0, 20]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(output_lengths)\n",
    "plt.axis([85,89,0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = 78\n",
    "hindi = 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = []\n",
    "for i in range(len(input_lengths)):\n",
    "    if(input_lengths[i]<75 and output_lengths[i]<85):\n",
    "        line1 = line1 + [lines[i]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2856\n"
     ]
    }
   ],
   "source": [
    "print(len(line1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted the histogram of the length of the input sentences and choose the length that makes the most sense. \n",
    "\n",
    "The reason we don't want sentences that are too long is because the computation becomes trickier for longer sentences and the performance also degrades. However we also want as many sentences in our dataset as possible.\n",
    "\n",
    "Thus it is important to choose the right length and discard sentences longer than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same for the lengths of the output sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 2869  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [(line[0]) for line in line1]\n",
    "input_texts.append('\\tplay.\\n')\n",
    "target_texts = [(line[1]) for line in line1]\n",
    "target_texts.append('\\t.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = set()\n",
    "target_characters = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in input_texts:\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "for target_text in target_texts:\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(input_characters))\n",
    "print(len(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2857\n",
      "Number of unique input tokens: 72\n",
      "Number of unique output tokens: 92\n",
      "Max sequence length for inputs: 74\n",
      "Max sequence length for outputs: 82\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:',num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            \n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2284 samples, validate on 572 samples\n",
      "Epoch 1/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0888 - val_loss: 2.0590\n",
      "Epoch 2/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0885 - val_loss: 2.0789\n",
      "Epoch 3/200\n",
      "2284/2284 [==============================] - 25s 11ms/step - loss: 0.0873 - val_loss: 2.0944\n",
      "Epoch 4/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0868 - val_loss: 2.0969\n",
      "Epoch 5/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0872 - val_loss: 2.1025\n",
      "Epoch 6/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0865 - val_loss: 2.1182\n",
      "Epoch 7/200\n",
      "2284/2284 [==============================] - 30s 13ms/step - loss: 0.0858 - val_loss: 2.0976\n",
      "Epoch 8/200\n",
      "2284/2284 [==============================] - 29s 13ms/step - loss: 0.0848 - val_loss: 2.0960\n",
      "Epoch 9/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0841 - val_loss: 2.1197\n",
      "Epoch 10/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0837 - val_loss: 2.1053\n",
      "Epoch 11/200\n",
      "2284/2284 [==============================] - 29s 13ms/step - loss: 0.0829 - val_loss: 2.1273\n",
      "Epoch 12/200\n",
      "2284/2284 [==============================] - 29s 13ms/step - loss: 0.0826 - val_loss: 2.1009\n",
      "Epoch 13/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0819 - val_loss: 2.1237\n",
      "Epoch 14/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0808 - val_loss: 2.1181\n",
      "Epoch 15/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0805 - val_loss: 2.1277\n",
      "Epoch 16/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0808 - val_loss: 2.1459\n",
      "Epoch 17/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0800 - val_loss: 2.1456\n",
      "Epoch 18/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0792 - val_loss: 2.1498\n",
      "Epoch 19/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0782 - val_loss: 2.1645\n",
      "Epoch 20/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0780 - val_loss: 2.1733\n",
      "Epoch 21/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0767 - val_loss: 2.1705\n",
      "Epoch 22/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0766 - val_loss: 2.1664\n",
      "Epoch 23/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0762 - val_loss: 2.1539\n",
      "Epoch 24/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0758 - val_loss: 2.1462\n",
      "Epoch 25/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0755 - val_loss: 2.1708\n",
      "Epoch 26/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0751 - val_loss: 2.1817\n",
      "Epoch 27/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0740 - val_loss: 2.1939\n",
      "Epoch 28/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0742 - val_loss: 2.1989\n",
      "Epoch 29/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0731 - val_loss: 2.1781\n",
      "Epoch 30/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0729 - val_loss: 2.2009\n",
      "Epoch 31/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0723 - val_loss: 2.1996\n",
      "Epoch 32/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0722 - val_loss: 2.2141\n",
      "Epoch 33/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0716 - val_loss: 2.2116\n",
      "Epoch 34/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0713 - val_loss: 2.2043\n",
      "Epoch 35/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0703 - val_loss: 2.2149\n",
      "Epoch 36/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0696 - val_loss: 2.2135\n",
      "Epoch 37/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0695 - val_loss: 2.2258\n",
      "Epoch 38/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0690 - val_loss: 2.2338\n",
      "Epoch 39/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0686 - val_loss: 2.2216\n",
      "Epoch 40/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0684 - val_loss: 2.2258\n",
      "Epoch 41/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0680 - val_loss: 2.2218\n",
      "Epoch 42/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0672 - val_loss: 2.2302\n",
      "Epoch 43/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0664 - val_loss: 2.2398\n",
      "Epoch 44/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0664 - val_loss: 2.2283\n",
      "Epoch 45/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0656 - val_loss: 2.2379\n",
      "Epoch 46/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0660 - val_loss: 2.2485\n",
      "Epoch 47/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0648 - val_loss: 2.2613\n",
      "Epoch 48/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0643 - val_loss: 2.2299\n",
      "Epoch 49/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0636 - val_loss: 2.2566\n",
      "Epoch 50/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0638 - val_loss: 2.2671\n",
      "Epoch 51/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0633 - val_loss: 2.2611\n",
      "Epoch 52/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0625 - val_loss: 2.2593\n",
      "Epoch 53/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0620 - val_loss: 2.2695\n",
      "Epoch 54/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0615 - val_loss: 2.2824\n",
      "Epoch 55/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0613 - val_loss: 2.2700\n",
      "Epoch 56/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0612 - val_loss: 2.2701\n",
      "Epoch 57/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0607 - val_loss: 2.2693\n",
      "Epoch 58/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0610 - val_loss: 2.2953\n",
      "Epoch 59/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0600 - val_loss: 2.2874\n",
      "Epoch 60/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0589 - val_loss: 2.2848\n",
      "Epoch 61/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0586 - val_loss: 2.2922\n",
      "Epoch 62/200\n",
      "2284/2284 [==============================] - 29s 12ms/step - loss: 0.0583 - val_loss: 2.2965\n",
      "Epoch 63/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0581 - val_loss: 2.3222\n",
      "Epoch 64/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0572 - val_loss: 2.3123\n",
      "Epoch 65/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0566 - val_loss: 2.3195\n",
      "Epoch 66/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0564 - val_loss: 2.3254\n",
      "Epoch 67/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0560 - val_loss: 2.3128\n",
      "Epoch 68/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0556 - val_loss: 2.3297\n",
      "Epoch 69/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0555 - val_loss: 2.3404\n",
      "Epoch 70/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0550 - val_loss: 2.3313\n",
      "Epoch 71/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0540 - val_loss: 2.3650\n",
      "Epoch 72/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0534 - val_loss: 2.3436\n",
      "Epoch 73/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0540 - val_loss: 2.3265\n",
      "Epoch 74/200\n",
      "2284/2284 [==============================] - 73s 32ms/step - loss: 0.0530 - val_loss: 2.3692\n",
      "Epoch 75/200\n",
      "2284/2284 [==============================] - 47s 21ms/step - loss: 0.0530 - val_loss: 2.3434\n",
      "Epoch 76/200\n",
      "2284/2284 [==============================] - 23s 10ms/step - loss: 0.0524 - val_loss: 2.3480\n",
      "Epoch 77/200\n",
      "2284/2284 [==============================] - 32s 14ms/step - loss: 0.0519 - val_loss: 2.3702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0522 - val_loss: 2.3768\n",
      "Epoch 79/200\n",
      "2284/2284 [==============================] - 29s 13ms/step - loss: 0.0514 - val_loss: 2.3679\n",
      "Epoch 80/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0510 - val_loss: 2.3763\n",
      "Epoch 81/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0507 - val_loss: 2.3774\n",
      "Epoch 82/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0504 - val_loss: 2.4008\n",
      "Epoch 83/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0495 - val_loss: 2.3936\n",
      "Epoch 84/200\n",
      "2284/2284 [==============================] - 31s 13ms/step - loss: 0.0492 - val_loss: 2.3738\n",
      "Epoch 85/200\n",
      "2284/2284 [==============================] - 31s 13ms/step - loss: 0.0491 - val_loss: 2.3938\n",
      "Epoch 86/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0480 - val_loss: 2.4003\n",
      "Epoch 87/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0489 - val_loss: 2.4059\n",
      "Epoch 88/200\n",
      "2284/2284 [==============================] - 32s 14ms/step - loss: 0.0474 - val_loss: 2.4083\n",
      "Epoch 89/200\n",
      "2284/2284 [==============================] - 31s 13ms/step - loss: 0.0478 - val_loss: 2.3935\n",
      "Epoch 90/200\n",
      "2284/2284 [==============================] - 30s 13ms/step - loss: 0.0474 - val_loss: 2.3989\n",
      "Epoch 91/200\n",
      "2284/2284 [==============================] - 31s 13ms/step - loss: 0.0466 - val_loss: 2.3987\n",
      "Epoch 92/200\n",
      "2284/2284 [==============================] - 29s 13ms/step - loss: 0.0476 - val_loss: 2.4163\n",
      "Epoch 93/200\n",
      "2284/2284 [==============================] - 25s 11ms/step - loss: 0.0464 - val_loss: 2.4158\n",
      "Epoch 94/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0462 - val_loss: 2.4055\n",
      "Epoch 95/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0455 - val_loss: 2.4301\n",
      "Epoch 96/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0451 - val_loss: 2.4078\n",
      "Epoch 97/200\n",
      "2284/2284 [==============================] - 32s 14ms/step - loss: 0.0443 - val_loss: 2.4196\n",
      "Epoch 98/200\n",
      "2284/2284 [==============================] - 30s 13ms/step - loss: 0.0442 - val_loss: 2.4080\n",
      "Epoch 99/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0443 - val_loss: 2.4319\n",
      "Epoch 100/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0440 - val_loss: 2.4147\n",
      "Epoch 101/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0435 - val_loss: 2.4216\n",
      "Epoch 102/200\n",
      "2284/2284 [==============================] - 31s 14ms/step - loss: 0.0426 - val_loss: 2.4490\n",
      "Epoch 103/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0421 - val_loss: 2.4281\n",
      "Epoch 104/200\n",
      "2284/2284 [==============================] - 25s 11ms/step - loss: 0.0425 - val_loss: 2.4547\n",
      "Epoch 105/200\n",
      "2284/2284 [==============================] - 31s 13ms/step - loss: 0.0424 - val_loss: 2.4456\n",
      "Epoch 106/200\n",
      "2284/2284 [==============================] - 30s 13ms/step - loss: 0.0413 - val_loss: 2.4357\n",
      "Epoch 107/200\n",
      "2284/2284 [==============================] - 30s 13ms/step - loss: 0.0409 - val_loss: 2.4580\n",
      "Epoch 108/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0407 - val_loss: 2.4438\n",
      "Epoch 109/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0408 - val_loss: 2.4416\n",
      "Epoch 110/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0400 - val_loss: 2.4295\n",
      "Epoch 111/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0396 - val_loss: 2.4451\n",
      "Epoch 112/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0392 - val_loss: 2.4932\n",
      "Epoch 113/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0395 - val_loss: 2.4690\n",
      "Epoch 114/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0389 - val_loss: 2.4399\n",
      "Epoch 115/200\n",
      "2284/2284 [==============================] - 25s 11ms/step - loss: 0.0385 - val_loss: 2.4646\n",
      "Epoch 116/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0383 - val_loss: 2.4785\n",
      "Epoch 117/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0376 - val_loss: 2.4584\n",
      "Epoch 118/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0380 - val_loss: 2.4699\n",
      "Epoch 119/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0371 - val_loss: 2.4751\n",
      "Epoch 120/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0368 - val_loss: 2.4703\n",
      "Epoch 121/200\n",
      "2284/2284 [==============================] - 25s 11ms/step - loss: 0.0360 - val_loss: 2.4818\n",
      "Epoch 122/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0364 - val_loss: 2.4777\n",
      "Epoch 123/200\n",
      "2284/2284 [==============================] - 30s 13ms/step - loss: 0.0358 - val_loss: 2.5158\n",
      "Epoch 124/200\n",
      "2284/2284 [==============================] - 30s 13ms/step - loss: 0.0358 - val_loss: 2.4811\n",
      "Epoch 125/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0350 - val_loss: 2.4920\n",
      "Epoch 126/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0355 - val_loss: 2.5094\n",
      "Epoch 127/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0349 - val_loss: 2.4731\n",
      "Epoch 128/200\n",
      "2284/2284 [==============================] - 24s 11ms/step - loss: 0.0340 - val_loss: 2.4820\n",
      "Epoch 129/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0340 - val_loss: 2.4836\n",
      "Epoch 130/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0339 - val_loss: 2.5218\n",
      "Epoch 131/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0336 - val_loss: 2.4962\n",
      "Epoch 132/200\n",
      "2284/2284 [==============================] - 31s 14ms/step - loss: 0.0329 - val_loss: 2.5409\n",
      "Epoch 133/200\n",
      "2284/2284 [==============================] - 32s 14ms/step - loss: 0.0325 - val_loss: 2.4994\n",
      "Epoch 134/200\n",
      "2284/2284 [==============================] - 30s 13ms/step - loss: 0.0318 - val_loss: 2.5078\n",
      "Epoch 135/200\n",
      "2284/2284 [==============================] - 31s 13ms/step - loss: 0.0323 - val_loss: 2.5270\n",
      "Epoch 136/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0324 - val_loss: 2.5617\n",
      "Epoch 137/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0320 - val_loss: 2.5475\n",
      "Epoch 138/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0316 - val_loss: 2.5297\n",
      "Epoch 139/200\n",
      "2284/2284 [==============================] - 29s 12ms/step - loss: 0.0312 - val_loss: 2.5436\n",
      "Epoch 140/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0311 - val_loss: 2.5254\n",
      "Epoch 141/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0303 - val_loss: 2.5171\n",
      "Epoch 142/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0307 - val_loss: 2.5245\n",
      "Epoch 143/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0303 - val_loss: 2.5605\n",
      "Epoch 144/200\n",
      "2284/2284 [==============================] - 29s 12ms/step - loss: 0.0295 - val_loss: 2.5661\n",
      "Epoch 145/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0295 - val_loss: 2.5165\n",
      "Epoch 146/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0297 - val_loss: 2.5415\n",
      "Epoch 147/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0295 - val_loss: 2.5638\n",
      "Epoch 148/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0295 - val_loss: 2.5879\n",
      "Epoch 149/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0306 - val_loss: 2.5565\n",
      "Epoch 150/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0285 - val_loss: 2.5589\n",
      "Epoch 151/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0284 - val_loss: 2.5493\n",
      "Epoch 152/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0283 - val_loss: 2.5666\n",
      "Epoch 153/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0278 - val_loss: 2.5546\n",
      "Epoch 154/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0266 - val_loss: 2.5806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "2284/2284 [==============================] - 25s 11ms/step - loss: 0.0276 - val_loss: 2.5739\n",
      "Epoch 156/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0272 - val_loss: 2.5688\n",
      "Epoch 157/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0269 - val_loss: 2.5903\n",
      "Epoch 158/200\n",
      "2284/2284 [==============================] - 25s 11ms/step - loss: 0.0269 - val_loss: 2.5790\n",
      "Epoch 159/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0264 - val_loss: 2.5911\n",
      "Epoch 160/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0259 - val_loss: 2.5975\n",
      "Epoch 161/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0259 - val_loss: 2.5786\n",
      "Epoch 162/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0251 - val_loss: 2.5854\n",
      "Epoch 163/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0259 - val_loss: 2.5911\n",
      "Epoch 164/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0251 - val_loss: 2.5804\n",
      "Epoch 165/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0258 - val_loss: 2.5949\n",
      "Epoch 166/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0245 - val_loss: 2.5851\n",
      "Epoch 167/200\n",
      "2284/2284 [==============================] - 26s 11ms/step - loss: 0.0248 - val_loss: 2.6031\n",
      "Epoch 168/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0243 - val_loss: 2.5947\n",
      "Epoch 169/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0246 - val_loss: 2.6107\n",
      "Epoch 170/200\n",
      "2284/2284 [==============================] - 25s 11ms/step - loss: 0.0245 - val_loss: 2.6181\n",
      "Epoch 171/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0246 - val_loss: 2.5974\n",
      "Epoch 172/200\n",
      "2284/2284 [==============================] - 28s 12ms/step - loss: 0.0240 - val_loss: 2.5841\n",
      "Epoch 173/200\n",
      "2284/2284 [==============================] - 26s 12ms/step - loss: 0.0232 - val_loss: 2.6110\n",
      "Epoch 174/200\n",
      "2284/2284 [==============================] - 27s 12ms/step - loss: 0.0237 - val_loss: 2.6306\n",
      "Epoch 175/200\n",
      "2284/2284 [==============================] - 63s 28ms/step - loss: 0.0231 - val_loss: 2.6167\n",
      "Epoch 176/200\n",
      "2284/2284 [==============================] - 50s 22ms/step - loss: 0.0231 - val_loss: 2.6114\n",
      "Epoch 177/200\n",
      "2284/2284 [==============================] - 45s 20ms/step - loss: 0.0229 - val_loss: 2.6231\n",
      "Epoch 178/200\n",
      "2284/2284 [==============================] - 46s 20ms/step - loss: 0.0225 - val_loss: 2.6143\n",
      "Epoch 179/200\n",
      "2284/2284 [==============================] - 45s 20ms/step - loss: 0.0229 - val_loss: 2.6210\n",
      "Epoch 180/200\n",
      "2284/2284 [==============================] - 44s 19ms/step - loss: 0.0224 - val_loss: 2.6127\n",
      "Epoch 181/200\n",
      "2284/2284 [==============================] - 42s 18ms/step - loss: 0.0220 - val_loss: 2.6290\n",
      "Epoch 182/200\n",
      "2284/2284 [==============================] - 42s 19ms/step - loss: 0.0220 - val_loss: 2.6355\n",
      "Epoch 183/200\n",
      "2284/2284 [==============================] - 42s 18ms/step - loss: 0.0225 - val_loss: 2.6293\n",
      "Epoch 184/200\n",
      "2284/2284 [==============================] - 42s 18ms/step - loss: 0.0215 - val_loss: 2.6186\n",
      "Epoch 185/200\n",
      "2284/2284 [==============================] - 42s 18ms/step - loss: 0.0212 - val_loss: 2.6261\n",
      "Epoch 186/200\n",
      "2284/2284 [==============================] - 43s 19ms/step - loss: 0.0215 - val_loss: 2.6164\n",
      "Epoch 187/200\n",
      "2284/2284 [==============================] - 42s 18ms/step - loss: 0.0209 - val_loss: 2.6411\n",
      "Epoch 188/200\n",
      "2284/2284 [==============================] - 42s 18ms/step - loss: 0.0213 - val_loss: 2.6617\n",
      "Epoch 189/200\n",
      "2284/2284 [==============================] - 43s 19ms/step - loss: 0.0208 - val_loss: 2.6509\n",
      "Epoch 190/200\n",
      "2284/2284 [==============================] - 41s 18ms/step - loss: 0.0210 - val_loss: 2.6484\n",
      "Epoch 191/200\n",
      "2284/2284 [==============================] - 42s 19ms/step - loss: 0.0205 - val_loss: 2.6344\n",
      "Epoch 192/200\n",
      "2284/2284 [==============================] - 43s 19ms/step - loss: 0.0204 - val_loss: 2.6503\n",
      "Epoch 193/200\n",
      "2284/2284 [==============================] - 43s 19ms/step - loss: 0.0201 - val_loss: 2.6501\n",
      "Epoch 194/200\n",
      "2284/2284 [==============================] - 43s 19ms/step - loss: 0.0205 - val_loss: 2.6523\n",
      "Epoch 195/200\n",
      "2284/2284 [==============================] - 43s 19ms/step - loss: 0.0202 - val_loss: 2.6626\n",
      "Epoch 196/200\n",
      "2284/2284 [==============================] - 43s 19ms/step - loss: 0.0196 - val_loss: 2.6421\n",
      "Epoch 197/200\n",
      "2284/2284 [==============================] - 42s 19ms/step - loss: 0.0198 - val_loss: 2.6424\n",
      "Epoch 198/200\n",
      "2284/2284 [==============================] - 44s 19ms/step - loss: 0.0196 - val_loss: 2.6429\n",
      "Epoch 199/200\n",
      "2284/2284 [==============================] - 44s 19ms/step - loss: 0.0199 - val_loss: 2.6496\n",
      "Epoch 200/200\n",
      "2284/2284 [==============================] - 44s 19ms/step - loss: 0.0195 - val_loss: 2.6445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12636ae45c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,batch_size=batch_size,epochs=epochs,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aane\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('s2s.h5')\n",
    "model.save_weights('seq2seq.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=load_model('s2s.h5')\n",
    "model.load_weights('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "               stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "-\n",
      "\tHelp!\n",
      "\n",
      "बचाओ!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(0,1):\n",
    "    \n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    print(input_seq)\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print(input_texts[seq_index])\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]]\n",
      "-\n",
      "\tI learned to drive a car and got a driver's license when I was eighteen.\n",
      "\n",
      "मैंने चाबियों को अपने बटुए के साथ छोड़ दिया।\n",
      "\n",
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "-\n",
      "\tplay.\n",
      "\n",
      "आराम से आओ।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2855,2857):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    print(input_seq)\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print(input_texts[seq_index])\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
